
<p><big><big><b>
    ラビットチャレンジ E資格受験講座
</b></big></big></p>
<p><big><big><b>
    応用数学要約
</b></big></big></p>
<ol>
<p><big>
   <li>線形代数</li>
</p></big>
<p>
行列は連立一次方程式の解法の研究の過程で見出された手法であり、連立一次方程式でいう<br>
各文字（たとえばｘ）の係数を並べたものである。これはベクトルを羅列したものとみなすことができる。<br>
行列の積においては左側の行成分と右側の列成分のベクトル同士の内積の結果が次の要素として射影される。<br>
掛けられる前のベクトルと積の結果が同じ方向を示すベクトルが存在する。これを固有ベクトルといい、その係数を固有値と呼ぶ。<br>
固有値分解は、任意の行列を1)固有値、2) 固有ベクトル行列、3) その逆行列によって構成され累乗計算に役立つ。<br>
行列が正方行列でない場合は特異値分解によって表現できる。<br>
講義の画像処理のサンプルに示されたように、固有値分解や特異値分解によって、ただの数字の羅列からベクトルと大きさ分解し、<br>
解析対象の特徴を掴む効果がありそうである。行列の各成分の方向がいずれかに従属した場合、行列式は０となり逆行列を得ることができない。<br>
</p>
<p><big>
   <li>確率統計</li>
</p></big>
<p>
<p>
    〇　頻度論とベイズ論
</p>
<p>頻度確率：母集団が決まっており、母集団からどのくらいの頻度で発生するのかの度合いを表現したもの。<br>
ベイズ確率：得られたデータから、任意の母集団にどのくらい確率で発生するかを示したもの。<br>
ベイズ確率には母集団が定まっておらず、データ自体が分布を持つ。（確率変数である。）<br>
</p>
<p>
    〇 ベルヌーイ分布とマルチヌーイ分布
</p>
<p>ベルヌーイ分布：結果が0か1かについて試行回数1回のときに表される分布<br>
マルチヌーイ分布：カテゴリカル分布であり、0か1ではなく任意のカテゴリ数の発生確率と分布について考える。（多項分布）<br>
</p>
<p>
    〇　二項分布とガウス分布
</p>
<p>二項分布：ベルヌーイ分布の多試行版であり、例えば計数値の分布に適用される。<br>
ガウス分布：連続値に関する分布であり、例えば計量値の分布に適用される。<br>
</p>
<p><big>
   <li>情報理論</li>
</p></big>
<p>
<p>
    〇　自己情報量
</p>
<p>確率を表現する別手法のようなもの。確率をLOGで表現した値。底が2のときbit、eのときnatと表現される。<br>
</p>
<p>
    〇　シャノンエントロピー
</p>
<p>自己情報量の期待値<br>
</p>
<p>
    〇　カルバック・ライブラー・ダイバージェンス
</p>
<p>同じ事象・確率変数下での確率分布の違いを表す。自己情報量を使用することで減算によって解を得られる。<br>
</p>
<p>
    〇　交差エントロピー
</p>
<p>KLダイバージェンスの一部を取り出したものであり、2つの確率分布の違いを表現する手法の一つ。<br>
機械学習において損失関数として使用される。二乗誤差関数に比べて、数値の乖離が大きい場合、<br>
交差エントロピーを使うほうがより学習スピードが早いという特徴がある。<br>
</p>
<br>
</ol>
<p><big><big><b>
    機械学習要約
</b></big></big></p>
<ol>
<p><big>
   <li>線形回帰モデル</li>
</big></p>
<p>ある入力データから出力を予想する回帰問題の中で、入力と説明変数の次元数の線形結合によって出力するモデル。<br>
目的変数はスカラー値。パラメータとなる説明変数の係数を学習データからの最小二乗法によって算出し、モデルを推定する。
</p>
<p><big>
   <li>非線形回帰モデル</li>
</big></p>
<p>
線形回帰モデルは説明変数と推定パラメータ係数の積の線形結合によって表現するのに対し、非線形回帰は説明変数を<br>
既知の非線形関数（基底関数）に変換して、その非線形出力に係数(w)を掛けたものを線形結合によって表現する。<br>
パラメータは最小二乗法や最尤法によって推定する。モデル化にあたっては過学習に注意が必要である。<br>
交差検証法によって汎化性能の高いモデルを選定できる。<br>
</p>
<p><big>
   <li>ロジスティック回帰モデル</li>
</big></p>
<p>
目的変数が0か1かのモデル。ロジスティック線形回帰の場合は、線形回帰モデルの出力をシグモイド関数の入力として与え、<br>
0～1範囲の数値表現に変換する。例えば、その結果が0.5以上なら1、0.5未満なら0とするなど閾値を設けて判定する。<br>
シグモイド関数の大きな特徴として、微分値をシグモイド関数自身で表現できることが挙げられる。<br>
この性質は後の深層学習の誤差逆伝播法に非常に大きな役割を担う。<br>
尤度関数はパラメータを推定する際に用いられる指標である。データを固定しパラメータを変化させる中で尤度関数を最大化する推定方法を最尤推定という。<br>
</p>
<p><big>
   <li>主成分分析</li>
</big></p>
<p>
多変量のデータをより少数個の指標に圧縮する手法の一つとして知られる。得られたデータから分散共分散行列を計算し、固有値と固有ベクトルを計算する。<br>
固有値の大きい固有ベクトルから順に第1主成分、第2主成分．．．となる。データの分散値から計算される寄与率は、<br>
得られた主成分がどの程度データを説明できるか表すことができ、どこまでの主成分を採用するかを判断する指標となる。<br>
</p>
<p><big>
   <li>アルゴリズム</li>
</big></p>
<p>
    〇　k近傍法
</p>
<p>
あるデータに対して、その点近傍のk個のデータの属するクラスが最も多いクラスに分類する手法。kの値によって結果が変わり、大きくなるほど決定境界が滑らかになる。
</p>
<p>
    〇　k-means法
</p>
<p>
クラスタリング（グループ分けのようなもの）手法である。アルゴリズムとしては、①各クラスタの中心の初期値を与え、②各データ点にクラスタ中心との距離を計算し、<br>
最も近いクラスタを割り当て、③各クラスタの平均中心を計算し、④②③を値が収束するまで繰り返すというもの。Meansは平均を表す。
</p>
<p><big>
   <li>サポートベクターマシーン</li>
</big></p>
<p>
2クラス分類のための機械学習手法の一つで、線形回帰モデル（線形判別関数）の出力を入力とした符号関数の正負によってクラス分けを行う。<br>
2クラスそれぞれが最も近い点サポートベクターに対してマージンが最大となるような線形判別関数の係数を求める。<br>
線形判別では分離できないような場合は特徴空間に写像して、特徴空間内の線形判別で考える。直接非線形な判別関数（カーネルトリック）を設定することもできる。<br>
</p>
</ol>
<br>
<p><big>
   機械学習実装演習レポートリンク
</big></p>
<p style="padding-left:2em">
線形回帰
</p>
<p  style="padding-left:2em">
    <a
        href="https://drive.google.com/file/d/1Y2j3kmm-y3CQSIoazVgTYWE86-yWxraV/view?usp=sharing"
    >
        https://drive.google.com/file/d/1Y2j3kmm-y3CQSIoazVgTYWE86-yWxraV/view?usp=sharing
    </a>
</p>
<p style="padding-left:2em">
非線形回帰
</p>
<p style="padding-left:2em">
    <a
        href="https://drive.google.com/file/d/16WJlIOB4hx-uqgnmOiHRufC6axitOB3v/view?usp=sharing"
    >
        https://drive.google.com/file/d/16WJlIOB4hx-uqgnmOiHRufC6axitOB3v/view?usp=sharing
    </a>
</p>
<p style="padding-left:2em">
ロジスティック回帰
</p>
<p style="padding-left:2em">
    <a
        href="https://drive.google.com/file/d/1s1QcZUfoqDzD9YffpijNz3k_3q1garPK/view?usp=sharing"
    >
        https://drive.google.com/file/d/1s1QcZUfoqDzD9YffpijNz3k_3q1garPK/view?usp=sharing
    </a>
</p>
<p style="padding-left:2em">
主成分分析
</p>
<p style="padding-left:2em">
    <a
        href="https://drive.google.com/file/d/1Bf-OFPiTXHi4DmPqXtVHXmUkPyMUpA-o/view?usp=sharing"
    >
        https://drive.google.com/file/d/1Bf-OFPiTXHi4DmPqXtVHXmUkPyMUpA-o/view?usp=sharing
    </a>
</p>
<p style="padding-left:2em">
k-means法
</p>
<p style="padding-left:2em">
    <a
        href="https://drive.google.com/file/d/1Bd2O8NZC1XHxLnuCnHgD9EJlYwLXnAkR/view?usp=sharing"
    >
        https://drive.google.com/file/d/1Bd2O8NZC1XHxLnuCnHgD9EJlYwLXnAkR/view?usp=sharing
    </a>
</p>
